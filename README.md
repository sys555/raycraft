# Raycraft

**Pure Ray-based Minecraft Gym Environment**

A lightweight, high-performance Minecraft reinforcement learning environment with zero HTTP overhead.

---

## ğŸŒŸ Features

- âš¡ **Pure Ray Architecture** - Direct RPC communication, no HTTP overhead
- ğŸš€ **Parallel Creation** - 10 environments in ~2 min vs 20 min serial
- ğŸ“ **Auto Path Isolation** - Each env gets `output/env-{uuid}/`
- ğŸ® **Gym-compliant** - Standard `reset()` and `step()` interface
- ğŸŒ **Distributed Ready** - Scale across multiple machines
- ğŸ¬ **Built-in Recording** - Automatic MP4 recording per episode

---

## ğŸš€ Quick Start

```python
from raycraft import MCRayClient

# Batch create 10 environments (parallel, ~2 min)
uuids = MCRayClient.create_batch(
    num_envs=10,
    config_path="configs/kill/kill_zombie_with_record.yaml"
)

# Connect to a specific environment
client = MCRayClient(uuid=uuids[0])

# Standard Gym interface
obs = client.reset()
result = client.step('[{"action": "forward"}]')
print(f"Reward: {result.reward}, Done: {result.done}")

# Recording saved to: output/env-{uuid[:8]}/episode_0.mp4
client.close()
```

---

## ğŸ“¦ Installation

```bash
# Clone with MineStudio submodule
git clone --recursive https://github.com/YOUR_ORG/raycraft
cd raycraft

# Install dependencies
pip install -e .

# Or manually init submodule
git submodule update --init --recursive
```

---

## ğŸ—ï¸ Architecture

```
User Code
  â””â”€â†’ MCRayClient.create_batch(10)
       â”œâ”€â†’ MCRayClient(uuid1) â”€â”€â”
       â”œâ”€â†’ MCRayClient(uuid2) â”€â”€â”¤
       â””â”€â†’ MCRayClient(uuid3) â”€â”€â”´â”€â†’ EnvPool â†’ MCEnvActor â†’ MCSimulator â†’ MinecraftSim
                                     (Global)   (Ray Actor)  (Env Logic)    (Minecraft)
```

**Key Components:**
- **MCRayClient** - User-facing interface with Gym API
- **EnvPool** - Global UUID-based environment registry
- **MCEnvActor** - Ray actor wrapping each environment instance
- **MCSimulator** - Environment logic adapter
- **MinecraftSim** - MineStudio's Minecraft simulation

---

## âš™ï¸ Configuration

Create YAML configs in `configs/`:

```yaml
# configs/my_task.yaml
defaults:
  - base
  - _self_

env:
  seed: 42
  preferred_spawn_biome: "forest"
  timestep_limit: 2000

text: "Kill 10 zombies"

custom_init_commands:
  - "/time set day"
  - "/give @p minecraft:diamond_sword 1"
```

---

## ğŸ“š Examples

See `examples/` directory:
- `mvp1_basic_test.py` - Basic usage and batch creation
- `test_batch_create.py` - Advanced batch operations
- `mvp1_migration_guide.py` - Migrating from other frameworks

---

## ğŸ¯ Use Cases

- **Reinforcement Learning** - Train agents in Minecraft
- **Curriculum Learning** - Gradually increase task difficulty
- **Multi-task Learning** - Train on diverse tasks in parallel
- **Distributed Training** - Scale across GPU clusters

---

## ğŸ“– Documentation

- [Architecture](docs/architecture.md) - Design principles and structure
- [Ray Design](docs/ray-design.md) - Pure Ray implementation details
- [API Reference](docs/api_reference.md) - Complete API documentation

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

---

## ğŸ“„ License

MIT License - see LICENSE file

---

## ğŸ™ Powered By

- [MineStudio](https://github.com/CraftJarvis/MineStudio) - Minecraft simulation engine
- [Ray](https://github.com/ray-project/ray) - Distributed computing framework
- [OpenAI Gym](https://github.com/openai/gym) - RL environment standard

---

## ğŸ“§ Contact

Issues: https://github.com/YOUR_ORG/raycraft/issues
